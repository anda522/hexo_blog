---
title: 《动手学深度学习》笔记
top: false
cover: false
toc: true
mathjax: true
abbrlink: 1060509989
date: 2023-01-15 19:36:00
password:
summary:
tags:
  - 学习总结
  - DeepLearning
categories:
  - 人工智能
---

>  本笔记主要参考[《动手学深度学习》](https://zh-v2.d2l.ai/index.html) 进行学习记录，不做无意义的书本内容摘抄。
>
> 课程网址：https://courses.d2l.ai/zh-v2/
>
> 使用深度学习框架为：Pytorch

内容不断更新中$\cdots \cdots$

# 2 预备知识

## 2.1 数据处理

## 2.2 线性代数

矩阵运算：矩阵点积、向量积、矩阵乘法

> 向量积非叉积，是矩阵与向量的积。下例可以看做$A$的行向量和向量的点积。
> $$
> A = 
> \begin{pmatrix}
> a_{11} & \cdots & a_{1m} \\\
> a_{21} & \cdots & a_{2m} \\\
> \vdots & \ddots & \vdots \\\
> a_{n1} & \cdots & a_{nm}
> \end{pmatrix} ,
> \overrightarrow x = 
> \begin{pmatrix}
> x_1 \\\
> \vdots \\\
> x_m
> \end{pmatrix}
> \\\
> A\overrightarrow x = 
> \begin{pmatrix}
> a_{11}x_1 + \cdots + a_{1m}x_m \\\
> \vdots \\\
> a_{n1}x_1 + \cdots + a_{nm}x_m
> \end{pmatrix}
> $$
> 

范数：

$L_1$范数是向量绝对值之和。
$$
\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.
$$
$L_2$范数是向量元素根的平方和。
$$
\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2},
$$
$L_p$范数是更一般的范数，公式如下：
$$
\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.
$$

## 2.3 微积分

# 3 线性神经网络

## 3.1 线性回归

## 3.2 Softmax回归

输出的匹配概率（非负，和为1）
$$
\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}
$$
我们选择最优可能的类别
$$
\mathop {argmax}_j \hat y_j = \mathop {argmax}_j o_j.
$$
损失函数，下式通常称为**交叉熵损失**。（真实概率中只有一个$y_i$为1，其余全部为0）
$$
l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.
$$

> 交叉熵通常来衡量两个概率的区别：
> $$
> H(p, q) = \sum \limits_i - p_i log(q_i)
> $$

损失函数的导数
$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$

## 3.3 损失函数

- 均方损失（L2 Loss）

$$
l(y, y{'}) =\frac{1}{2}(y - y{'})^2
$$

- 绝对值损失（L1 Loss）

$$
l(y,y{'}) = |y - y{'}|
$$

- Huber's Robust Loss 鲁棒损失

$$
l(y, y{'}) = 
\begin{cases}
|y - y{'}| & if |y - y{'}| \gt 1 \\\
\frac{1}{2}(y - y{'})^2 & otherwise
\end{cases}
$$



# 4 多层感知机

## 4.1 多层感知机

单层感知机：

- 回归输出实数
- 解决二分类问题

单层感知机函数，$\sigma$是激活函数
$$
o = \sigma (\langle \mathbf{w} , \mathbf{x} \rangle + b )
$$
多层感知机：

- 激活函数需要是非线性：如果为线性的话，输出就仍然是线性的，等价于单层感知机

- 使用隐藏层和激活函数来得到非线性模型
- 使用Softmax来处理多分类
- 超参数为隐藏层数和各个隐藏层大小

单隐藏层：
$$
\mathbf{h} = \mathbf{W_1X+b_1} \\\
o = \mathbf{w_2^T h } + b_2
$$


激活函数：

- sigmoid函数，将输出投影到$(0, 1)$

$$
sigmoid(x) = \frac{1}{1 + e ^{-x}}
$$

- Tanh函数，将输出投影到$(-1, 1)$

$$
tanh(x) = \frac{1 - e^{-2x}}{1 + e ^ {-2x}}
$$

- ReLU激活函数

$$
ReLU(x) = max(x, 0)
$$

## 4.2 模型选择

训练误差：模型在训练数据集上得到的误差

泛化误差：模型在新数据集上得到的误差
