---
title: 《动手学深度学习》笔记
top: false
cover: false
toc: true
mathjax: true
abbrlink: 1060509989
date: 2023-01-15 19:36:00
password:
summary:
tags:
  - 学习总结
  - DeepLearning
categories:
  - 知识总结
---

>  本笔记主要参考[《动手学深度学习》](https://zh-v2.d2l.ai/index.html) 进行学习记录，不做无意义的书本内容摘抄。
>
> 课程网址：https://courses.d2l.ai/zh-v2/
>
> 使用深度学习框架为：Pytorch

内容不断更新中$\cdots \cdots$

# 1 预备知识

## 1.1 数据处理

## 1.2 线性代数

矩阵运算：矩阵点积、向量积、矩阵乘法

> 向量积非叉积，是矩阵与向量的积。下例可以看做$A$的行向量和向量的点积。
> $$
> A = 
> \begin{pmatrix}
> a_{11} & \cdots & a_{1m} \\\
> a_{21} & \cdots & a_{2m} \\\
> \vdots & \ddots & \vdots \\\
> a_{n1} & \cdots & a_{nm}
> \end{pmatrix} ,
> \overrightarrow x = 
> \begin{pmatrix}
> x_1 \\\
> \vdots \\\
> x_m
> \end{pmatrix}
> \\\
> A\overrightarrow x = 
> \begin{pmatrix}
> a_{11}x_1 + \cdots + a_{1m}x_m \\\
> \vdots \\\
> a_{n1}x_1 + \cdots + a_{nm}x_m
> \end{pmatrix}
> $$
> 

范数：

$L_1$范数是向量绝对值之和。
$$
\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.
$$
$L_2$范数是向量元素根的平方和。
$$
\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2},
$$
$L_p$范数是更一般的范数，公式如下：
$$
\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.
$$

## 1.3 微积分

# 2 线性神经网络

## 2.1 线性回归

## 2.2 Softmax回归

输出的匹配概率（非负，和为1）
$$
\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}
$$
我们选择最优可能的类别
$$
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.
$$
损失函数，下式通常称为**交叉熵损失**。（真实概率中只有一个$y_i$为1，其余全部为0）
$$
l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.
$$

> 交叉熵通常来衡量两个概率的区别：
> $$
> H(p, q) = \sum \limits_i - p_i log(q_i)
> $$

损失函数的导数
$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$

## 2.3 损失函数

- 均方损失（L2 Loss）

$$
l(y, y{'}) =\frac{1}{2}(y - y{'})^2
$$

- 绝对值损失（L1 Loss）

$$
l(y,y{'}) = |y - y{'}|
$$

- Huber's Robust Loss 鲁棒损失

$$
l(y, y{'}) = 
\begin{cases}
|y - y{'}| & if |y - y{'}| \gt 1 \\\
\frac{1}{2}(y - y{'})^2 & otherwise
\end{cases}
$$



