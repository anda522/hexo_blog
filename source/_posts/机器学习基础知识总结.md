---
title: 机器学习基础知识总结
top: false
cover: false
toc: true
mathjax: true
tags:
  - 学习总结
categories:
  - 知识总结
abbrlink: 808139430
date: 2022-12-27 17:04:31
password:
summary:
---

# 机器学习基础概念知识总结

# 机器学习环境搭建

- 一般使用anaconda搭建python虚拟环境（miniconda占的空间应该小一点，这个也可以）
- 使用工具库一般有科学计算库numpy，数据处理库pandas，绘图matplotlib等，需要了解相关用法

- ......

# 线性回归 Linear Regression

## 1 概述

线性回归类似高中的**线性规划**题目。线性回归要做的是就是找到一个数学公式能相对较完美地把所有自变量组合（加减乘除）起来，得到的结果和目标接近。

线性回归分为一元线性回归和多元线性回归。

## 2 一元线性回归

### 2.1 构造回归方程

有n组数据，自变量（特征值） $x(x_1,x_2,...,x_n)$ 与因变量（目标值） $y(y_1,y_2,...,y_n)$ ，我们需要找到一个线性关系，使他们之间尽可能满足： $f(x) =ax+b$ ，这个就是构建的一元线性方程。

![一元线性回归](808139430/1.jpg)

线性回归的目标就是让 $f(X)$ 与 $y$ 之间的差距最小，也就是权重$a$和偏置$b$取什么值的时候$f(X)$和$y$最接近。

### 2.2 构造损失函数

损失函数是来度量模型预测值与真实值不一样的程度的，或者说度量预测错误的程度，损失函数值越小，模型就越好。

在回归问题中，误差平方和是回归任务中最常用的性能度量。这里就可以令损失函数$L(a,b)$等于误差平方和。

则损失函数为: $L(a, b) = \sum \limits_{i = 1}^{n}(f(x_i) - y_i)^2$

### 2.3 确定参数

我们需要通过最小的损失函数得到最佳的参数 $a$ 和 $b$ 。一般使用**最小二乘法**。
$$
a = \frac{\sum \limits_{i=1}^{n}x_iy_i - n \overline x \overline y}{\sum \limits_{i=1}^{n}x_i^2 - n \overline x ^ 2}
\\\
b = \overline y - a \overline x
$$


## 3 多元线性回归

多元线性回归类似一元

回归方程： $y = a_1 x_1 + a_2 x_2 + a_3 x_3 + ... + a_n x_n + b$

对所有的数据统一用矩阵形式表示：
$$
y^{(i)} = \theta ^ T x ^ {(i)} + \varepsilon^{(i)} \ (1)
$$

> $\varepsilon$ 误差代表真实值和预测值之间的差异
>
> 误差 $\varepsilon ^{(i)}$ 是独立并具有相同的分布，服从均值为 0 方差为 $\theta ^ 2$ 的高斯分布

损失函数：$L(a_1, a_2, ..., a_n, b) = \sum_{i = 1}^{n}(f(x_i) - y_i)^2$

高斯分布的概率函数：
$$
p(x) = \frac{1}{\sqrt {2 \pi} \sigma} \exp{(-\frac{x^2}{2 \sigma ^ 2})} \  (2)
$$
将`(1)`带入`(2)`得到**预测值成为真实值的概率**函数： 
$$
p(y ^ {(i)} | x ^ {(i)}; \theta) = \frac{1}{\sqrt {2 \pi} \sigma} \exp{(-\frac{(y^{(i)} - \theta ^ T x ^ {(i)})^2}{2 \sigma ^ 2})}
$$
似然函数：（什么样的参数计算出来的误差最小，即与实际值最接近）
$$
L(\theta) = \prod \limits_{i = 1}^{m} p(y ^ {(i)} | x ^ {(i)}; \theta) = \prod \limits_{i=1}^{m}\frac{1}{\sqrt {2 \pi} \sigma} \exp{(-\frac{(y^{(i)} - \theta ^ T x ^ {(i)})^2}{2 \sigma ^ 2})}
$$
对数似然法：（将乘法转化为加法），之后需要用极大似然估计方法求解
$$
ln L(\theta) = ln \prod \limits_{i=1}^{m}\frac{1}{\sqrt {2 \pi} \sigma} \exp{(-\frac{(y^{(i)} - \theta ^ T x ^ {(i)})^2}{2 \sigma ^ 2})}
$$
展开化简：
$$
ln L(\theta) = \sum \limits_{i = 1}^{m}ln \frac{1}{\sqrt {2 \pi} \sigma} \exp{(-\frac{(y^{(i)} - \theta ^ T x ^ {(i)})^2}{2 \sigma ^ 2})} 
\\\
= mln \frac{1}{\sqrt {2 \pi} \sigma} - \frac{1}{\sigma^2} \frac{1}{2} \sum \limits _{i = 1}^{m} (y^{(i)} - \theta ^ T x ^ {(i)})^2
$$

目标：让似然函数越大越好（极大似然估计），即让$J(\theta)$越小越好（可以使用**最小二乘法**求解）
$$
J(\theta) = \frac{1}{2} \sum \limits _{i = 1}^{m} (y^{(i)} - \theta ^ T x ^ {(i)})^2
$$
![最小二乘法分析](808139430/image-20221227173603912.png)

![评估方法](808139430/image-20221227173755095.png)

## 4 梯度下降

梯度下降法（gradient descent）是一种常用的一阶（first-order）优化方法。主要解决求最小值问题，其基本思想在于不断地逼近最优点，每一步的优化方向就是梯度的方向。

### 4.1 梯度下降方法

- 批量梯度下降

容易得到最优解，但是由于每次考虑所有样本，速度很慢。

- 随机梯度下降

每次找一个样本，迭代速度很快，但不一定每次都朝着收敛的方向。

- 小批量梯度下降

每次更新一小部分数据来算，比较实用。

### 4.2 其他参数

- 学习率：更新的步长

![学习率的影响](808139430/image-20221228161134519.png)

- 批处理数量

一般`batch_size`选择32，64，128等，有时候会考虑内存和效率。

# 逻辑回归

逻辑回归是一个经典的二分类算法。

## 1 sigmoid函数

$$
g(z) = \frac{1}{1 + e ^ {-z}}, z \in R
$$

![sigmoid函数图像](808139430/image-20221228161724190.png)

> 将任意的输入映射到了$[0, 1]$区间中，在线性回归中可以得到一个预测值，再将该值映射到sigmoid函数中，这样就可以完成由值到概率的转换，这就是分类任务。

## 2 逻辑回归求解

预测函数：
$$
h_{\theta}(x) = g(\theta ^ T x) = \frac{1}{1 + e ^ {-\theta^T x}} \\\
其中 \theta_0 + \theta_1 x_1 + ... + \theta_n x_n = \sum \limits_{i = 1}^n \theta_i x_i = \theta ^ T x
$$
分类任务：
$$
\begin{cases}
P(y = 1|x; \theta) = h_\theta(x) \\\
P(y = 0|x; \theta) = 1 - h_\theta(x)
\end{cases}
\Rightarrow
P(y | x; \theta) = (h_\theta(x)) ^ y (1 - h_\theta(x)) ^ {1 - y}
$$
对于二分类任务（0， 1），整合后，`y`取0只保留$(1 - h_\theta(x)) ^ {1 - y}$ ，`y`取1只保留 $(h_\theta(x)) ^ y$ 。

似然函数：
$$
L(\theta) = \prod \limits_{i = 1}^m P(y_i | x_i; \theta) = \prod \limits_{i = 1}^m (h_\theta(x_i)) ^ y_i (1 - h_\theta(x_i)) ^ {1 - y_i}
$$
对数似然法，即求$l(\theta)$ 的最大值：
$$
l(\theta) = logL(\theta) = \sum \limits_{i = 1} ^m (y_i log h_\theta(x_i) + (1 - y_i) log (1 - h_\theta(x_i)))
$$
将上述函数转化为求最小值，同时系数乘上一个常数，即求$J(\theta) = -\frac{1}{m}l(\theta)$ 的最小值，转化为梯度下降问题：
$$
J(\theta) = -\frac{1}{m}l(\theta)
$$
![求导过程](808139430/image-20221228164952743.png)

上述过程即求出了偏导的方向，有了更新方向就可以进行参数更新： $\alpha$代表学习率
$$
\theta_j = \theta_j - \alpha \frac{1}{m} \sum \limits_{i = 1} ^ m (h_\theta(x_i) - y_i)x_i^j
$$

> 减法是代表用的梯度下降，整体除以`m`是考虑了所有的m个样本。

多分类问题：

![多分类](808139430/image-20221228171310763.png)

# 模型评估标准

## 1 回归模型评估

### 1.1 平均绝对误差（Mean Absolute Error，MAE）

平均绝对误差就是指预测值与真实值之间平均相差多大
$$
MAE = \frac{1}{m}\sum \limits _{i = 1}^m \lvert f_i - y_i \rvert
$$

### 1.2 均方误差（Mean Squared Error，MSE）

观测值与真值偏差的平方和与观测次数的比值
$$
MSE = \frac{1}{m} \sum \limits_{i = 1}^m(f_i - y_i)^2
$$
这也是线性回归中最常用的损失函数，线性回归过程中尽量让该损失函数最小。那么模型之间的对比也可以用它来比较。

MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。

### 1.3 R-square（决定系数）

$$
R^2 = 1 - \frac{\sum(Y_{actual} - Y_{predict})^2}{\sum(Y_{actual} - Y_{mean})^2}
$$

### 1.4 Adjusted R-Square（校正决定系数）

$$
R^2_{adjusted} = 1 - \frac{(1 - R^2)(n - 1)}{n - p - 1}
$$



n为样本数量，p为特征数量

消除了样本数量和特征数量的影响

### 1.5 交叉验证

我们有一个总的数据集，将总数据集切分，例如，将数据分为训练集（80%）和测试集（20%），训练集用来训练model，测试集用来最终的测试。

训练集还再平均进行切分为3份（标号为1、2、3）。

> 测试集和训练集的比例自己定。

交叉验证就是在训练集中，采用2份数据来训练，用另一份数据来验证训练出的模型的参数，进行3次。

即：1 + 2来训练，3验证；2 + 3来训练，1来验证；1 + 3来训练，2来验证。

为了让模型的评估效果比较好，最后将3次的参数取平均值。

> 无论分类还是回归模型，都可以利用交叉验证，进行模型评估
>
> sklearn模块中有交叉验证函数，例如`sklearn.cross_validation` 中的 `train_testsplit` 函数

交叉验证主要是为了防止某一部分数据比较简单，导致模型的效果比较高。

## 2 分类模型评估

### 2.1 准确率、精确率、召回率、f1_score

- 准确率（Accuracy）的定义是：对于给定的测试集，分类模型正确分类的样本数与总样本数之比；

- 精确率（Precision）的定义是：对于给定测试集的某一个类别，分类模型预测正确的比例，或者说：分类模型预测的正样本中有多少是真正的正样本；

- 召回率（Recall）的定义为：对于给定测试集的某一个类别，样本中的正类有多少被分类模型预测正确；

  > 假设有1000个人，其中990个人正常，有10个人患有癌症，模型旨在预测哪些人是患有癌症的。
  >
  > 如果模型预测1000个人中都是正常的，没有癌症患者，那么可以说模型的精度是$\frac{990}{1000}=0.99$。虽然精度很高，但是都是正样本，没有负样本，模型是无用的，因为一个患者都没有找到。因此无法用精度来评估模型，而是使用recall召回率来评估。

- F1_score，在理想情况下，我们希望模型的精确率越高越好，同时召回率也越高越高，但是，现实情况往往事与愿违，在现实情况下，精确率和召回率像是坐在跷跷板上一样，往往出现一个值升高，另一个值降低，那么，有没有一个指标来综合考虑精确率和召回率了，这个指标就是F值。F值的计算公式为：
  $$
  F = \frac{(a ^ 2 + 1) \times P \times R}{a ^ 2 \times (P + R)}
  $$

  > P: Precision， R: Recall, a：权重因子
  >
  > 当a=1时，F值便是F1值，代表精确率和召回率的权重是一样的，是最常用的一种评价指标。
  >
  > F1的计算公式为：$F1 = \frac{2 \times P \times R}{P + R}$

### 2.2 混淆矩阵

混淆矩阵也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。

具体评价指标有总体精度、制图精度、用户精度等，这些精度指标从不同的侧面反映了图像分类的精度。

下面是一个混淆矩阵，`Actual`代表真实值，`Predicted`代表预测值，预测的是标签号（因为是分类任务，主要对标签进行分类）。

> 下面是我对TP、TN、FP、FN四个值的理解（助记）
>
> TP：预测正确，预测成1
>
> TN：预测正确，预测成0
>
> FP：预测错误，预测成1
>
> FN：预测错误，预测成0

![混淆矩阵](808139430/image-20221230142020907.png)

可以通过上面四个值计算相应的评估值，见下图。

![混淆矩阵计算评估指标](808139430/image-20221230143921870.png)

# 回归模型相关技巧

## 1 下采样和上采样

在分类问题的数据中，很容易出现正反数据集数量存在极大的差距，这类数据直接用于训练不利于模型的构架，所以我们需要对数据进行些许处理。

很容易想到，合理的数据集应该是正反数据集数量应接近，那就存在两种策略：

下采样策略：把数量多的减少到与数量少的相近

上（过）采样策略：把数量少的增加到与数量多的相近

- 下采样：

![img](808139430/2.png)

- 上采样：SMOTE算法

**步骤：**

（1）对于少数类中每一个样本x，以**欧氏距离（两点之间距离）**为标准计算它到少数类样本集中所有样本的距离，得到其`k`近邻（所有距离排序后前`k`小的距离）

（2）根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本x，从其k近邻中随机选择若干个样本，假设选择的近邻为xn

（3）对于每一个随机选出的近邻xn，分别与原样本按照如下的公式构建新的样本。
$$
x_{new} = x + rand(0, 1) \times (\widetilde x - x)
$$

> $(\widetilde x - x)$ 相当于距离 $d_i$ （欧几里得距离），那么每个 $d_i$ 都可以生成一个新的数据。

![SMOTE算法原理图](808139430/image-20221230161807567.png)

## 2 正则化惩罚

加上了正则化项能在一定程度上避免过拟合

# 决策树

## 1 概述

### 1.1 定义

决策树是一种解决分类问题的算法，决策树算法采用树形结构，使用层层推理来实现最终的分类。

决策树即可以做分类，也可以做回归。它主要分为两种：**分类树** 和 **回归树**。

### 1.2 决策树算法

- 第一个决策树算法: CLS （Concept Learning System）
- 使决策树受到关注、成为机器学习主流技术的算法: ID3
- 最常用的决策树算法: C4.5
- 可以用于回归任务的决策树算法: CART （Classification and Regression Tree）
- 基于决策树的最强大算法: RF （Random Forest）

### 1.3 结构

决策树由下面几种元素构成：

- 根节点：包含样本的全集（全部训练数据）
- 内部节点：对应特征属性测试
- 叶节点：代表决策的结果

![决策树结构](808139430/image-20221230164038527.png)

决策树学习的**目的**是为了产生一棵泛化能力强的决策树

## 2 决策树构建

### 2.1 构建过程

整体策略：自上而下分而治之

决策树的构建过程就是一个**自根至叶的递归过程**， 在每个中间结点寻找一个**划分**属性。

大致过程：

- 开始：构建根节点，所有训练数据都放在根节点，选择x个最优特征，按着这一特征将训练数据集分割成子集，进入子节点。
- 所有子集按内部节点的属性递归地进行分割。
- 如果这些子集已经能够被基本正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点去。
- 每个子集都被分到叶节点上，即都有了明确的类，这样就生成了一颗决策树。

递归的三种停止条件：

- 当前结点包含的样本全属于同一类别，无需划分；
- 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分;
- 当前结点包含的样本集合为空，不能划分。

### 2.2 特征选择

**信息熵**：随机变量的不确定性。
$$
H(X) = - \sum p_i log_2 p_i \hspace{2em} \text{i = 1, 2, ..., n}
$$

> 例：
>
> A集合 $[1, 1, 1, 1, 1, 1, 1, 1, 2, 2]$
>
> B集合$[1, 2, 3, 4, 5, 6, 7, 8, 9, 1]$
>
> A集合熵值低于B集合熵值，因为A集合中只有两种类别，B集合中类别比较多（结构比较乱），熵值就会比较大

**信息增益：** 表示特征X使得类Y的不确定性减少的程度（熵值减少），即当前划分对信息熵所造成的变化。

信息增益越大，表示特征a来划分所减少的熵最大，即提升最大，应当作为根节点。

## 3 决策树算法

### 3.1 ID3（信息增益）

下面是基于信息增益的ID3算法的实例：

我们有14天的数据，4个特征条件：**天气，温度，湿度，是否有风**。最终结果是去玩不玩。

![数据](808139430/image-20221231110826312.png)

![划分方式](808139430/image-20221231110929844.png)

上面有四种划分方式，我们需要判断谁来当根节点，根据的主要就是信息增益这个指标。下面计算信息增益来判断根节点。

总的数据中，9天玩，5天不玩，熵值为：
$$
-\frac{9}{14}log_2 \frac{9}{14} - \frac{5}{14}log_2 \frac{5}{14} = 0.940
$$
然后对4个特征逐个分析：

- outlook

  - `outlook = sunny`时，熵值为0.971，取值为sunny的概率为 $\frac{5}{14}$
  - `outlook = overcast`时，熵值为0，取值为overcast的概率为 $\frac{4}{14}$
  - `outlook = rainy`时，熵值为0.971，取值为rainy的概率为 $\frac{5}{14}$

  熵值为：
  $$
  \frac{5}{14} \times 0.971 + \frac{4}{14} \times 0 + \frac{5}{14} \times 0.971 = 0.693
  $$
  信息增益：系统熵值从0.940下降到0.693，增益为0.247。

- 其他特征按照相同方法来做

计算出所有的信息增益之后，选择**有最大的信息增益的特征**作为根节点。

### 3.2 C4.5（信息增益率）

> 基于信息增益的决策树算法会有哪些问题：
>
> 如果有一个特征：id，代表样本的编号，以上述数据为例，id为从1到14，如果计算id特征的根节点，发现信息增益是最大的，因为每一个子节点的信息熵值都为0。

信息增益率：（解决了ID3的问题，考虑自身熵，信息增益除以自身熵）
$$
\frac{G}{H(x)} \hspace{2em} \text{G:信息增益, H(x):熵值}
$$

### 3.3 CART（GINI系数） 

使用基尼系数作为衡量标准。
$$
Gini(p) = \sum \limits _{k = 1}^K p_k (1 - p_k) = 1 - \sum \limits _{k = 1}^K p_k^2
$$


## 3 决策树剪枝

### 3.1 预剪枝

在建立决策树边的时候进行剪枝的操作，比较使用实用。

剪枝策略：

- 限制深度
- 限制叶子结点个数
- 限制叶子结点样本数
- 限制信息增益量等。

### 3.2 后剪枝

建立完决策树后进行剪枝操作。

## 4 连续值和缺失值处理

- 连续值属性可取数值不是有限的，不能根据连续树形的可取值对节点进行划分。常见做法是：**二分法**对其进行离散化。

- 现实应用中，经常会遇到属性值`缺失`现象仅使用无缺失的样例，这是对数据的极大浪费使用带缺失值的样例，需解决：

  - 如何进行划分属性选择?
  - 给定划分属性，若样本在该属性上的值缺失，如何进行划分?

  基本思路：**样本赋权，权重划分**

# 集成算法

## 1 概述

集成算法：Ensemble Learning

Bagging：训练多个分类器取平均 
$$
f(x) = \frac{1}{M} \sum \limits_{m = 1}^M f_m(x)
$$
Boosting：从弱学习器开始加强，通过加权来训练。
$$
F_m(x) = F_{m - 1}(x) + argmin_h \sum \limits_{i = 1}^n L(y_i, F_{m - 1}(x_i) + h(x_i))
$$
Stacking：聚合多个分类或回归模型。

## 2 Bagging模型-随机森林

其实就是并行训练一堆分类器（每个分类器互相独立）。典型代表为随机森林（多个决策树并行放在一起）。

> 随机指的是：数据随机采样，特征随机选择
>
> 每个分类器喂的数据随机，数据的特征数随机。二重随机性，会让每个树基本都不一样，最终的结果也不一样。

随机森林优势：

- 可以处理高维度（feature多）数据，不用做特征选择
- 训练完之后，可以给出那些feature比较重要
- 容易做成并行化方法，速度快
- 可以进行可视化展示，便于分析

## 3 Boosting模型

提升模型典型代表：AdaBoost，XgBoost

AdaBoost：会根据前一次的分类效果调整数据权重

## 4 Stacking模型

堆叠模型：可以堆叠各种各样的分类器（KNN，SVM，RF等）

分阶段进行：第一阶段得出各自的结果，第二阶段再利用前一阶段结果进行训练。

# 贝叶斯算法

贝叶斯公式：
$$
P(A | B) = \frac{P(B|A)P(A)}{P(B)}
$$

## 1.1 实例：拼写纠正

用户输入一个不在词典中的单词，需要猜测用户真正想输入的单词。

我们要求的是`P(我们猜测用户想输入的单词|用户实际输入的单词)`

假设用户实际输入的单词为`D`（Data）

我们有多个猜测：`P(h1 | D), P(h2 | D)`， 方便后续计算，统一为`P(h | D)`
$$
P(h | D) = \frac{P(h) P(D | h)}{P(D)}
$$

> $P(h)$为单词在语料库中出现的概率（出现次数 / 总次数），我们叫做**先验概率**，这个概率可以算出来。
>
> $P(D|h)$ 为我们将一个正确的词输入错误的概率。

对于所有的猜测，$P(D)$ 都是一样的，所以可以忽略这个常数。

则
$$
P(h|D) \varpropto P(h)P(D|h)
$$

>$P(D|h)$可以根据某种指标来判定，可以看键盘上字母的编辑距离来算概率等等。

如果计算出来多个结果预测概率是一样的，那么就可以使用**先验概率**来进行判断谁最优先。

## 1.2 拼写检查器实现

原理：
$$
argmaxc \ P(A|B) = argmaxc \  \frac{P(B|A) P(A)}{P(B)}
$$




$P(A|B)$：待求值，用户本想输入B的前提下，错输成A的概率

$P(A)$：文章中出现正确单词A的概率

$P(B|A)$：用户本想输入A的前提下，错输成B的概率

$P(B)$：文章中出现正确单词B的概率

$argmaxc$：用来枚举所有可能的A，并选取概率最大的那个

拼写检查器就是，输入一个单词，先判断这个单词是否存在于语料库中（是否正确），如果不在（可能语料库中没有，或者拼写错误），则需要根据编辑距离进行检查修正。

`big.txt`文件：https://wwwi.lanzouo.com/i9s8t0ju5qzg

```python
import re, collections

#  将所有大写字母转化为小写，并且去掉特殊字符
def words(text): return re.findall('[a-z]+', text.lower())

def train(features):
    # 遇到从来没有见过的新词但语料库中未包含，概率模型中希望返回一个很小的概率，故出现次数设置为1
    model = collections.defaultdict(lambda: 1)
    for f in features:
        model[f] += 1
    return model


NWORDS = train(words(open('big.txt').read()))  # 词频
alphabet = 'abcdefghijklmnopqrstuvwxyz'

# 编辑距离为1的单词
def edits1(word):
    n = len(word)
    return set([word[0: i] + word[i + 1:] for i in range(n)] +   # deletion
               [word[0: i] + word[i + 1] + word[i] + word[i + 2:] for i in range(n - 1)] +  # transportation
               [word[0: i] + c + word[i + 1: ] for i in range(n) for c in alphabet] +  # alteration
               [word[0: i] + c + word[i: ] for i in range(n + 1) for c in alphabet])  # insertion


# 编辑距离为2 的单词
def edits2(word):
    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))


# 将那些正确的词作为候选词
def known(words):
    return set(w for w in words if w in NWORDS)


# 检查器函数，先判断是不是正确的拼写形式，如果不是则选出编辑距离为1的单词……
def correct(word):
    candidates = known([word]) or known(edits1(word)) or known(edits2(word)) or [word]
    return max(candidates, key=lambda w: NWORDS[w])


print(correct('mach'))
```

## 1.3 新闻分类

之后用到了再补，短时间不会写。

# SVM支持向量机

## 1 概述

Support Vector Machine是一种二分类模型，它的基本模型是定义在特征空间上的**间隔最大的线性分类器**

SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。如下图所示， $wx+b=0$ 即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。

![支持向量机](808139430/image-20230102144808362.png)

## 2 推导

### 2.1 距离

正常三维条件下点$(x_0, y_0, z_0)$到平面$Ax + By + Cz + D = 0$的距离公式（高中知识）：
$$
\frac{\vert Ax_0 + By_0 + Cz_0 + D \vert}{\sqrt{A^2 + B^2 + C^2}}
$$
推导分析过程：

平面方程： $ax + by + cz = d$ ，平面外一点$P(x_0, y_0, z_0)$

![示意图](808139430/image-20230102153203878.png)

PQ垂直平面，即为求PQ的长度，但不知Q点的具体数据。

故构造一个平面上的点$P^{'}(x_1, y_1, z_1)$，问题即转化为求$\overrightarrow {P^{'}P}$ 在法向量N上面的分量，即$\overrightarrow {P^{'}P}$ 与N相同方向的单位向量的点积。

![示意图](808139430/image-20230102153230935.png)

设距离为D。

![距离公式推导](808139430/1203675-20180109152428254-718844217.png)

现在考虑一般情况：

求平面外一点 $x$ 到平面$w^T x + b = 0$ 的距离：

> 结论：平面$Ax + By + Cz + D = 0$的法向量为$(A, B, C)$

![示意图](808139430/image-20230102154354917.png)

同上述原理：

距离就为
$$
distance(x, b, w) = \vert \frac{w^T}{\vert w \vert}(x - x^{'}) \vert = \frac{1}{\vert w \vert} \vert w^Tx + b \vert
$$

> 上述公式进行了代入，将$x^{'}$代入平面方程得$w^Tx^{'} = -b$

### 2.2 数据

数据集：$(x_1, y_1)(x_2, y_2)...(x_n, y_n)$

$Y$ 为样本的类别：当$X$ 为正例时，$Y = +1$，当$X$为负例时，$Y = -1$

决策方程：$y(x) = w^T \Phi(x) + b$ （其中$\Phi(x)$是对数据做了变换）
$$
\begin{cases}
y(x_i) > 0 \Leftrightarrow y_i = +1 \\\
y(x_i) < 0 \Leftrightarrow y_i = -1
\end{cases}
\Longrightarrow
y_i y(x_i) > 0
$$

### 2.3 目标函数

将点到直线距离进行转化（化简）：
$$
\frac{y_i \cdot (w^T \cdot \Phi(x) + b)}{\vert w \vert}
$$

> $y_i y(x_i) > 0$ 直接乘上$y_i$ 将绝对值去掉，$|y_i| = 1$，并不影响值大小

放缩变换：对于决策方程（w, b）可以通过放缩变换是的其结果值$|Y| \geq 1$ ，则
$$
y_i \cdot (w^T \cdot \Phi(x_i) + b) \geq 1
$$

> 缩放之前w和b有无数组解，缩放之后w和b只有一组解。

优化目标：
$$
\mathop{arg\  max} \limits_{w, b} \bigg\{ \frac{1}{|w|} \mathop{min} \limits_i \Big \{ y_i \cdot (w^T \cdot \Phi(x_i) + b)\Big \} \bigg\}
$$

> $\mathop{min} \limits_i \Big \{ y_i \cdot (w^T \cdot \Phi(x_i) + b) \Big \}$ 是求所有样本点到平面的最小距离的那个点
>
> $arg\ max$ 是**最大化到平面最小距离的点的距离**
>
> 由于$y_i \cdot (w^T \cdot \Phi(x_i) + b) \geq 1$， 故最小值为1，只需要考虑 $\mathop{arg\  max} \limits_{w, b} \frac{1}{|w|}$

### 2.4 目标函数求解

