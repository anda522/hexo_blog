---
title: 机器学习基础知识总结
top: false
cover: false
toc: true
mathjax: true
tags:
  - 学习总结
categories:
  - 知识总结
abbrlink: 808139430
date: 2022-12-27 17:04:31
password:
summary:
---

# 机器学习基础概念知识总结

# 机器学习环境搭建

- 一般使用anaconda搭建python虚拟环境（miniconda占的空间应该小一点，这个也可以）
- 使用工具库一般有科学计算库numpy，数据处理库pandas，绘图matplotlib等，需要了解相关用法

- ......

# 线性回归 Linear Regression

## 1 概述

线性回归类似高中的**线性规划**题目。线性回归要做的是就是找到一个数学公式能相对较完美地把所有自变量组合（加减乘除）起来，得到的结果和目标接近。

线性回归分为一元线性回归和多元线性回归。

## 2 一元线性回归

### 2.1 构造回归方程

有n组数据，自变量（特征值） $x(x_1,x_2,...,x_n)$ 与因变量（目标值） $y(y_1,y_2,...,y_n)$ ，我们需要找到一个线性关系，使他们之间尽可能满足： $f(x) =ax+b$ ，这个就是构建的一元线性方程。

![一元线性回归](808139430/1.jpg)

线性回归的目标就是让 $f(X)$ 与 $y$ 之间的差距最小，也就是权重$a$和偏置$b$取什么值的时候$f(X)$和$y$最接近。

### 2.2 构造损失函数

损失函数是来度量模型预测值与真实值不一样的程度的，或者说度量预测错误的程度，损失函数值越小，模型就越好。

在回归问题中，误差平方和是回归任务中最常用的性能度量。这里就可以令损失函数$L(a,b)$等于误差平方和。

则损失函数为: $L(a, b) = \sum \limits_{i = 1}^{n}(f(x_i) - y_i)^2$

### 2.3 确定参数

我们需要通过最小的损失函数得到最佳的参数 $a$ 和 $b$ 。一般使用**最小二乘法**。
$$
a = \frac{\sum \limits_{i=1}^{n}x_iy_i - n \overline x \overline y}{\sum \limits_{i=1}^{n}x_i^2 - n \overline x ^ 2}
\\\
b = \overline y - a \overline x
$$


## 3 多元线性回归

多元线性回归类似一元

回归方程： $y = a_1 x_1 + a_2 x_2 + a_3 x_3 + ... + a_n x_n + b$

对所有的数据统一用矩阵形式表示：
$$
y^{(i)} = \theta ^ T x ^ {(i)} + \varepsilon^{(i)} \ (1)
$$

> $\varepsilon$ 误差代表真实值和预测值之间的差异
>
> 误差 $\varepsilon ^{(i)}$ 是独立并具有相同的分布，服从均值为 0 方差为 $\theta ^ 2$ 的高斯分布

损失函数：$L(a_1, a_2, ..., a_n, b) = \sum_{i = 1}^{n}(f(x_i) - y_i)^2$

高斯分布的概率函数：
$$
p(x) = \frac{1}{\sqrt {2 \pi} \sigma} \exp{(-\frac{x^2}{2 \sigma ^ 2})} \  (2)
$$
将`(1)`带入`(2)`得到**预测值成为真实值的概率**函数： 
$$
p(y ^ {(i)} | x ^ {(i)}; \theta) = \frac{1}{\sqrt {2 \pi} \sigma} \exp{(-\frac{(y^{(i)} - \theta ^ T x ^ {(i)})^2}{2 \sigma ^ 2})}
$$
似然函数：（什么样的参数计算出来的误差最小，即与实际值最接近）
$$
L(\theta) = \prod \limits_{i = 1}^{m} p(y ^ {(i)} | x ^ {(i)}; \theta) = \prod \limits_{i=1}^{m}\frac{1}{\sqrt {2 \pi} \sigma} \exp{(-\frac{(y^{(i)} - \theta ^ T x ^ {(i)})^2}{2 \sigma ^ 2})}
$$
对数似然法：（将乘法转化为加法），之后需要用极大似然估计方法求解
$$
ln L(\theta) = ln \prod \limits_{i=1}^{m}\frac{1}{\sqrt {2 \pi} \sigma} \exp{(-\frac{(y^{(i)} - \theta ^ T x ^ {(i)})^2}{2 \sigma ^ 2})}
$$
展开化简：
$$
ln L(\theta) = \sum \limits_{i = 1}^{m}ln \frac{1}{\sqrt {2 \pi} \sigma} \exp{(-\frac{(y^{(i)} - \theta ^ T x ^ {(i)})^2}{2 \sigma ^ 2})} 
\\\
= mln \frac{1}{\sqrt {2 \pi} \sigma} - \frac{1}{\sigma^2} \frac{1}{2} \sum \limits _{i = 1}^{m} (y^{(i)} - \theta ^ T x ^ {(i)})^2
$$

目标：让似然函数越大越好（极大似然估计），即让$J(\theta)$越小越好（可以使用**最小二乘法**求解）
$$
J(\theta) = \frac{1}{2} \sum \limits _{i = 1}^{m} (y^{(i)} - \theta ^ T x ^ {(i)})^2
$$
![最小二乘法分析](808139430/image-20221227173603912.png)

![评估方法](808139430/image-20221227173755095.png)

## 4 梯度下降

梯度下降法（gradient descent）是一种常用的一阶（first-order）优化方法。主要解决求最小值问题，其基本思想在于不断地逼近最优点，每一步的优化方向就是梯度的方向。

### 4.1 梯度下降方法

- 批量梯度下降

容易得到最优解，但是由于每次考虑所有样本，速度很慢。

- 随机梯度下降

每次找一个样本，迭代速度很快，但不一定每次都朝着收敛的方向。

- 小批量梯度下降

每次更新一小部分数据来算，比较实用。

### 4.2 其他参数

- 学习率：更新的步长

![学习率的影响](808139430/image-20221228161134519.png)

- 批处理数量

一般`batch_size`选择32，64，128等，有时候会考虑内存和效率。

# 逻辑回归

逻辑回归是一个经典的二分类算法。

## 1 sigmoid函数

$$
g(z) = \frac{1}{1 + e ^ {-z}}, z \in R
$$

![sigmoid函数图像](808139430/image-20221228161724190.png)

> 将任意的输入映射到了$[0, 1]$区间中，在线性回归中可以得到一个预测值，再将该值映射到sigmoid函数中，这样就可以完成由值到概率的转换，这就是分类任务。

## 2 逻辑回归求解

预测函数：
$$
h_{\theta}(x) = g(\theta ^ T x) = \frac{1}{1 + e ^ {-\theta^T x}} \\\
其中 \theta_0 + \theta_1 x_1 + ... + \theta_n x_n = \sum \limits_{i = 1}^n \theta_i x_i = \theta ^ T x
$$
分类任务：
$$
\begin{cases}
P(y = 1|x; \theta) = h_\theta(x) \\\
P(y = 0|x; \theta) = 1 - h_\theta(x)
\end{cases}
\Rightarrow
P(y | x; \theta) = (h_\theta(x)) ^ y (1 - h_\theta(x)) ^ {1 - y}
$$
对于二分类任务（0， 1），整合后，`y`取0只保留$(1 - h_\theta(x)) ^ {1 - y}$ ，`y`取1只保留 $(h_\theta(x)) ^ y$ 。

似然函数：
$$
L(\theta) = \prod \limits_{i = 1}^m P(y_i | x_i; \theta) = \prod \limits_{i = 1}^m (h_\theta(x_i)) ^ y_i (1 - h_\theta(x_i)) ^ {1 - y_i}
$$
对数似然法，即求$l(\theta)$ 的最大值：
$$
l(\theta) = logL(\theta) = \sum \limits_{i = 1} ^m (y_i log h_\theta(x_i) + (1 - y_i) log (1 - h_\theta(x_i)))
$$
将上述函数转化为求最小值，同时系数乘上一个常数，即求$J(\theta) = -\frac{1}{m}l(\theta)$ 的最小值，转化为梯度下降问题：
$$
J(\theta) = -\frac{1}{m}l(\theta)
$$
![求导过程](808139430/image-20221228164952743.png)

上述过程即求出了偏导的方向，有了更新方向就可以进行参数更新： $\alpha$代表学习率
$$
\theta_j = \theta_j - \alpha \frac{1}{m} \sum \limits_{i = 1} ^ m (h_\theta(x_i) - y_i)x_i^j
$$

> 减法是代表用的梯度下降，整体除以`m`是考虑了所有的m个样本。

多分类问题：

![多分类](808139430/image-20221228171310763.png)
